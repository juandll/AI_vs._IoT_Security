{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BV4TQa2O0aHe"
      },
      "source": [
        "# Data Normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4j2M0MkIHxB"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn\n",
        "!pip install scikeras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbulXmcLy8Aw"
      },
      "outputs": [],
      "source": [
        "target_directory = \"/content/drive/MyDrive/TLSHandshakesTraffic/\"\n",
        "directories = [d for d in os.listdir(target_directory) if os.path.isdir(os.path.join(target_directory, d))]\n",
        "# Print all devices:\n",
        "print(\"List of devices traffic to pre-process:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSJNQQoEy_7z"
      },
      "outputs": [],
      "source": [
        "for directory in directories:\n",
        "  print(directory)\n",
        "  cl_file = target_directory+directory+'/ClientHello-new.csv'\n",
        "  existing_df = pd.read_csv(cl_file, header=None, delimiter=';')\n",
        "  if 5 in existing_df.columns:\n",
        "    pass\n",
        "  else:\n",
        "    existing_df[5] = 0\n",
        "  for index, row in existing_df.iterrows():\n",
        "    try:\n",
        "        value = float(row[4])\n",
        "        existing_df.at[index, 5] = value\n",
        "        existing_df.at[index, 4] = 0\n",
        "    except ValueError:\n",
        "        pass\n",
        "  sh_file = cl_file = target_directory+directory+'/ServerHello-new.csv'\n",
        "  sh_pd = pd.read_csv(cl_file, header=None, delimiter=';')\n",
        "  sh_len = len(sh_pd)\n",
        "  ch_len = len(existing_df)\n",
        "\n",
        "  next_column_number = len(existing_df.columns)\n",
        "  existing_df[next_column_number] = sh_pd[1].iloc[:ch_len % sh_len]\n",
        "  next_column_number = len(existing_df.columns)\n",
        "  existing_df[next_column_number] = sh_pd[2].iloc[:ch_len % sh_len]\n",
        "\n",
        "\n",
        "  new_column = pd.Series([directory] * len(existing_df))\n",
        "  next_column_number = len(existing_df.columns)\n",
        "  print(existing_df)\n",
        "\n",
        "\n",
        "  existing_df[next_column_number] = new_column\n",
        "  existing_df.to_csv(target_directory+'features.csv', mode='a', header=False, index=False, sep=';')\n",
        "print(\"Rows appended successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRgbJ8J-0xwI"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "files = '/content/drive/MyDrive/TLSHandshakesTraffic/features.csv'\n",
        "datass = pd.read_csv(files, header=None, delimiter=';')\n",
        "shuffled_data = datass.sample(frac=1, random_state=42)\n",
        "total_rows = len(shuffled_data)\n",
        "train_rows = int(0.7 * total_rows)\n",
        "\n",
        "train = shuffled_data[train_rows]\n",
        "test = shuffled_data[train_rows:]\n",
        "\n",
        "train.to_csv('/content/drive/MyDrive/TLSHandshakesTraffic/train_f.csv', header=None, index=False, sep=';')\n",
        "#test.to_csv('/content/drive/MyDrive/TLSHandshakesTraffic/test_f.csv', header=None, index=False, sep=';')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gK1jh4wYnjPh"
      },
      "outputs": [],
      "source": [
        "from ctypes import *\n",
        "\n",
        "def hex_to_float(hex_string):\n",
        "    i = int(hex_string, 16)  # Convert from hex to a Python integer\n",
        "    cp = pointer(c_int(i))   # Create a C integer pointer\n",
        "    fp = cast(cp, POINTER(c_float))  # Cast the int pointer to a float pointer\n",
        "    return fp.contents.value  # Dereference the pointer to get the float value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SftAjepgKcBk"
      },
      "source": [
        "# pre train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLD2CDcII5bd"
      },
      "outputs": [],
      "source": [
        "!pip install joblib\n",
        "!pip install optuna\n",
        "import re\n",
        "\n",
        "def extract_server_name(server_string):\n",
        "    if isinstance(server_string, str):\n",
        "        pattern = r\"servername='(.*?)'\"\n",
        "        server_names = re.findall(pattern, server_string)\n",
        "        return server_names[0] if server_names else None\n",
        "    elif hasattr(server_string, 'servername'):\n",
        "        return server_string.servername\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8qTSXPFPwfY",
        "outputId": "54157686-12d6-4920-8897-ce1c5466b20f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 18_74_2e_41_4d_35\n",
            "1 20_df_b9_5f_41_7e\n",
            "2 20_f8_5e_cc_18_1f\n",
            "3 0_71_47_c0_91_93\n",
            "4 0_e_f3_3b_85_e5\n",
            "5 0_21_cc_4d_ce_8c\n",
            "6 18_b4_30_c8_d8_28\n",
            "7 0_17_88_68_5f_61\n",
            "8 14_91_82_b4_4b_5f\n",
            "9 0_3_7f_4f_c6_b5\n",
            "10 68_db_f5_90_19_64\n",
            "11 34_29_8f_1e_41_25\n",
            "12 7c_c7_9_56_6e_48\n",
            "13 50_c7_bf_5a_2e_a0\n",
            "14 50_c7_bf_a0_f3_76\n",
            "15 38_8c_50_68_d7_5c\n",
            "16 24_fd_5b_4_1b_75\n",
            "17 6c_56_97_35_39_f4\n",
            "18 6c_72_20_c5_a_3f\n",
            "19 70_2c_1f_3b_36_53\n",
            "20 8_66_98_a2_21_9e\n",
            "21 98_84_e3_e4_35_bd\n",
            "22 b0_d5_9d_b9_f0_b4\n",
            "23 84_c0_ef_2f_42_cc\n",
            "24 88_de_a9_8_3_b9\n",
            "25 b0_ce_18_27_9f_e4\n",
            "26 b4_e6_2a_27_34_b7\n",
            "27 bc_dd_c2_38_d_32\n",
            "28 9c_8e_cd_a_33_1b\n",
            "29 b0_fc_d_c9_0_4c\n",
            "30 c0_97_27_81_67_99\n",
            "31 d4_a3_3d_6b_1e_97\n",
            "32 d8_f7_10_c3_34_e4\n",
            "33 c0_97_27_73_aa_38\n",
            "34 fc_a1_83_38_e0_2d\n",
            "35 ec_fa_bc_2e_81_f7\n",
            "36 d8_28_c9_10_b5_60\n",
            "37 dc_4f_22_5b_1a_a3\n",
            "38 f4_b8_5e_31_73_db\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-30da1d28cd38>:12: DtypeWarning: Columns (4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  train_data = pd.read_csv(file_train, header=None, delimiter=';')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0          [<ServerName  nametype=host_name namelen=28 se...\n",
            "1          [<ServerName  nametype=host_name namelen=28 se...\n",
            "2          [<ServerName  nametype=host_name namelen=28 se...\n",
            "3          [<ServerName  nametype=host_name namelen=28 se...\n",
            "4          [<ServerName  nametype=host_name namelen=19 se...\n",
            "                                 ...                        \n",
            "3218237                                                    0\n",
            "3218238                                                    0\n",
            "3218239                                                    0\n",
            "3218240                                                    0\n",
            "3218241                                                    0\n",
            "Name: 4, Length: 3218242, dtype: object\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "target_directory = \"/content/drive/MyDrive/TLSHandshakesTraffic/\"\n",
        "directories = [d for d in os.listdir(target_directory) if os.path.isdir(os.path.join(target_directory, d))]\n",
        "i=0\n",
        "for directory in directories:\n",
        "\n",
        "    print(str(i)+' '+directory)\n",
        "    i = i + 1\n",
        "from sklearn.model_selection import train_test_split\n",
        "file_train = '/content/drive/MyDrive/TLSHandshakesTraffic/features.csv'\n",
        "train_data = pd.read_csv(file_train, header=None, delimiter=';')\n",
        "\n",
        "y = train_data[8]\n",
        "print(train_data[4])\n",
        "\n",
        "# set the lenghts as same\n",
        "#train_data[2] = 64\n",
        "#train_data[3]=64\n",
        "#train_data[0]=64\n",
        "#train_data[5]=64\n",
        "\n",
        "train_data[1] = train_data[1].apply(lambda x: int(x, 16))\n",
        "train_data[4] = train_data[4].apply(lambda x: extract_server_name(x))\n",
        "\n",
        "unique_strings = train_data[4].unique()\n",
        "\n",
        "\n",
        "string_to_numeric = {}\n",
        "\n",
        "# Assign numeric representations to string values starting from 1\n",
        "for i, string in enumerate(unique_strings, start=1):\n",
        "    string_to_numeric[string] = i\n",
        "train_data[4] = train_data[4].map(string_to_numeric)\n",
        "train_data.fillna(0, inplace=True)\n",
        "train_data.replace('None', 0, inplace=True)\n",
        "train_data.replace(' ', 0, inplace=True)\n",
        "X = train_data.drop(columns=[8,6,4])\n",
        "X.columns = range(X.shape[1])\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_mapping = {device_name: label for label, device_name in enumerate(directories)}\n",
        "\n",
        "# assistant=1 Kitchen=2 Hub=3 Plug=4 Door=5 Smarttv= 6 Camera=7\n",
        "table = {\n",
        "    '18_74_2e_41_4d_35': 1,\n",
        "    '20_df_b9_5f_41_7e': 1,\n",
        "    '20_f8_5e_cc_18_1f': 2,\n",
        "    '0_71_47_c0_91_93': 1,\n",
        "    '0_e_f3_3b_85_e5': 3,\n",
        "    '0_21_cc_4d_ce_8c': 3,\n",
        "    '18_b4_30_c8_d8_28':4,\n",
        "    '0_17_88_68_5f_61': 3,\n",
        "    '14_91_82_b4_4b_5f': 4,\n",
        "    '0_3_7f_4f_c6_b5': 3,\n",
        "    '68_db_f5_90_19_64': 1,\n",
        "    '34_29_8f_1e_41_25': 5,\n",
        "    '7c_c7_9_56_6e_48': 5,\n",
        "    '50_c7_bf_5a_2e_a0': 4,\n",
        "    '50_c7_bf_a0_f3_76': 4,\n",
        "    '38_8c_50_68_d7_5c': 6,\n",
        "    '24_fd_5b_4_1b_75': 3,\n",
        "    '6c_56_97_35_39_f4': 6,\n",
        "    '6c_72_20_c5_a_3f': 6,\n",
        "    '70_2c_1f_3b_36_53':2,\n",
        "    '8_66_98_a2_21_9e': 6,\n",
        "    '98_84_e3_e4_35_bd': 5,\n",
        "    'b0_d5_9d_b9_f0_b4': 7,\n",
        "    '84_c0_ef_2f_42_cc': 6,\n",
        "    '88_de_a9_8_3_b9': 6,\n",
        "    'b0_ce_18_27_9f_e4': 3,\n",
        "    'b4_e6_2a_27_34_b7': 2,\n",
        "    'bc_dd_c2_38_d_32': 4,\n",
        "    '9c_8e_cd_a_33_1b': 7,\n",
        "    'b0_fc_d_c9_0_4c': 7,\n",
        "    'c0_97_27_81_67_99': 2,\n",
        "    'd4_a3_3d_6b_1e_97': 1,\n",
        "    'd8_f7_10_c3_34_e4': 1,\n",
        "    'c0_97_27_73_aa_38': 2,\n",
        "    'fc_a1_83_38_e0_2d': 1,\n",
        "    'ec_fa_bc_2e_81_f7':3,\n",
        "    'd8_28_c9_10_b5_60': 2,\n",
        "    'dc_4f_22_5b_1a_a3': 4,\n",
        "    'f4_b8_5e_31_73_db': 7,\n",
        "}\n",
        "\n",
        "\n",
        "tablefabric = {\n",
        "    '18_74_2e_41_4d_35': 1,\n",
        "    '20_df_b9_5f_41_7e': 2,\n",
        "    '20_f8_5e_cc_18_1f': 3,\n",
        "    '0_71_47_c0_91_93': 1,\n",
        "    '0_e_f3_3b_85_e5': 4,\n",
        "    '0_21_cc_4d_ce_8c': 5,\n",
        "    '18_b4_30_c8_d8_28':6,\n",
        "    '0_17_88_68_5f_61': 7,\n",
        "    '14_91_82_b4_4b_5f': 8,\n",
        "    '0_3_7f_4f_c6_b5': 9,\n",
        "    '68_db_f5_90_19_64': 1,\n",
        "    '34_29_8f_1e_41_25': 10,\n",
        "    '7c_c7_9_56_6e_48': 11,\n",
        "    '50_c7_bf_5a_2e_a0': 12,\n",
        "    '50_c7_bf_a0_f3_76': 12,\n",
        "    '38_8c_50_68_d7_5c': 13,\n",
        "    '24_fd_5b_4_1b_75': 14,\n",
        "    '6c_56_97_35_39_f4': 1,\n",
        "    '6c_72_20_c5_a_3f': 15,\n",
        "    '70_2c_1f_3b_36_53':16,\n",
        "    '8_66_98_a2_21_9e': 17,\n",
        "    '98_84_e3_e4_35_bd': 18,\n",
        "    'b0_d5_9d_b9_f0_b4': 19,\n",
        "    '84_c0_ef_2f_42_cc': 20,\n",
        "    '88_de_a9_8_3_b9': 21,\n",
        "    'b0_ce_18_27_9f_e4': 22,\n",
        "    'b4_e6_2a_27_34_b7': 13,\n",
        "    'bc_dd_c2_38_d_32': 23,\n",
        "    '9c_8e_cd_a_33_1b': 24,\n",
        "    'b0_fc_d_c9_0_4c': 1,\n",
        "    'c0_97_27_81_67_99': 20,\n",
        "    'd4_a3_3d_6b_1e_97': 17,\n",
        "    'd8_f7_10_c3_34_e4': 27,\n",
        "    'c0_97_27_73_aa_38': 20,\n",
        "    'fc_a1_83_38_e0_2d': 1,\n",
        "    'ec_fa_bc_2e_81_f7':26,\n",
        "    'd8_28_c9_10_b5_60': 27,\n",
        "    'dc_4f_22_5b_1a_a3': 26,\n",
        "    'f4_b8_5e_31_73_db': 28,\n",
        "}\n",
        "\n",
        "\n",
        "#By type of device\n",
        "encoded_y = [table[name] for name in y]\n",
        "\n",
        "#by manyfacturer\n",
        "#encoded_y = [tablefabric[name] for name in y]\n",
        "\n",
        "# For type of device or type of manufacturer\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, encoded_y, test_size=0.30, random_state=42)\n",
        "\n",
        "#for device\n",
        "#X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.30, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8QZPsXU79yz"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBBx4NIKAsvb"
      },
      "source": [
        "Best parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOE4b5tnAumF"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "import time\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Scale data\n",
        "sc = StandardScaler()\n",
        "X_tr = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_val)\n",
        "\n",
        "def objective_svm(trial):\n",
        "    # Hyperparameters\n",
        "    C = trial.suggest_loguniform(\"C\", 1e-3, 10)\n",
        "    tol = trial.suggest_loguniform(\"tol\", 1e-6, 1e-2)\n",
        "\n",
        "    # Model\n",
        "    svm = LinearSVC(random_state=0, tol=tol, C=C)\n",
        "    clf = CalibratedClassifierCV(svm)\n",
        "    model = OneVsRestClassifier(clf, n_jobs=-1)\n",
        "\n",
        "    #train\n",
        "    model.fit(X_tr, y_train)\n",
        "\n",
        "    y_pred_svm = model.predict(X_test)\n",
        "\n",
        "    return accuracy_score(y_val, y_pred_svm)\n",
        "\n",
        "# Optuna\n",
        "study_svm = optuna.create_study(direction=\"maximize\")\n",
        "study_svm.optimize(objective_svm, n_trials=3)\n",
        "\n",
        "best_params_svm = study_svm.best_params\n",
        "print(\"\\n Best SVM Hyperparameters:\", best_params_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7J0VJTTA05-"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cy29WmNDWO0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "best_C = 0.23218097496132672\n",
        "best_tol = 2.2313982919324327e-05\n",
        "\n",
        "sc = StandardScaler()\n",
        "X_tr = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_val)\n",
        "\n",
        "svm = LinearSVC(random_state=0, tol=best_tol, C=best_C)\n",
        "clf = CalibratedClassifierCV(svm)\n",
        "model = OneVsRestClassifier(clf, n_jobs=-1)\n",
        "\n",
        "model.fit(X_tr, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "precision = precision_score(y_val, y_pred, average=\"weighted\", zero_division=0)\n",
        "recall = recall_score(y_val, y_pred, average=\"weighted\", zero_division=0)\n",
        "f1 = f1_score(y_val, y_pred, average=\"weighted\", zero_division=0)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBWJ5lL_2Xdr"
      },
      "source": [
        "# Deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZIdzmOlBehs"
      },
      "source": [
        "Best parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iq5cn47HBfxa"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler, label_binarize\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "sc = StandardScaler()\n",
        "X_train2 = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_val)\n",
        "\n",
        "y_train_bin = label_binarize(y_train, classes=np.unique(y_train))\n",
        "y_val_bin = label_binarize(y_val, classes=np.unique(y_train))\n",
        "\n",
        "def objective_nn(trial):\n",
        "    # Hyperparameters\n",
        "    num_units = trial.suggest_int(\"units\", 16, 128, step=16)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
        "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n",
        "    epochs = trial.suggest_int(\"epochs\", 10, 50, step=5)\n",
        "\n",
        "    # Model\n",
        "    model = Sequential([\n",
        "        Dense(num_units, activation='relu', input_dim=X_train2.shape[1]),\n",
        "        Dense(num_units, activation='relu'),\n",
        "        Dense(y_train_bin.shape[1], activation='softmax')\n",
        "    ])\n",
        "\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    model.fit(X_train2, y_train_bin, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "\n",
        "    y_pred_prob = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "    y_val_true = np.argmax(y_val_bin, axis=1)\n",
        "\n",
        "    return accuracy_score(y_val_true, y_pred)\n",
        "\n",
        "# Optuna\n",
        "study_nn = optuna.create_study(direction=\"maximize\")\n",
        "study_nn.optimize(objective_nn, n_trials=3)\n",
        "\n",
        "best_params_nn = study_nn.best_params\n",
        "print(\"\\n Best Deep learning Hyperparameters:\", best_params_nn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "740cYjduBgE6"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nei-J-jJ2iAs"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy as np\n",
        "import numpy\n",
        "import time\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# changes depending on what is classifying, if tyoe of device or device manufacturer or device\n",
        "sc = StandardScaler()\n",
        "X_train2 = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_val)\n",
        "\n",
        "y_train_bin = label_binarize(y_train, classes=np.unique(y_train))\n",
        "y_val_bin = label_binarize(y_val, classes=np.unique(y_train))\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(64, activation='relu', input_dim=6))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(27, activation='softmax'))\n",
        "\n",
        "optimizer = Adam(learning_rate=0.0004274800244154512)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "start_time = time.time()\n",
        "model.fit(X_train2, y_train_bin, epochs=10, batch_size=64)\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Training time: done {elapsed_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFtOQWkY6zRk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, roc_curve, auc, precision_score, recall_score,f1_score\n",
        "start_time = time.time()\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred_bin = (y_pred_prob > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_val_bin, y_pred_bin)\n",
        "precision = precision_score(y_val_bin, y_pred_bin, average='micro')\n",
        "recall = recall_score(y_val_bin, y_pred_bin, average='micro')\n",
        "f1 = f1_score(y_val_bin, y_pred_bin, average='micro')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDQTwaR79Iyp"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUs9ODd9A4zV"
      },
      "source": [
        "Best params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMKer4i9A3v4"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "import time\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "sc = RobustScaler()\n",
        "X_train2 = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_val)\n",
        "\n",
        "def objective_rf(trial):\n",
        "\n",
        "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 300, step=50)\n",
        "    max_depth = trial.suggest_int(\"max_depth\", 5, 50, step=5)\n",
        "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
        "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
        "\n",
        "    clf = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        random_state=1,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    clf.fit(X_train2, y_train)\n",
        "    y_pred_rf = clf.predict(X_test)\n",
        "    return accuracy_score(y_val, y_pred_rf)\n",
        "\n",
        "study_rf = optuna.create_study(direction=\"maximize\")\n",
        "study_rf.optimize(objective_rf, n_trials=20)\n",
        "best_params_rf = study_rf.best_params_\n",
        "print(\"\\n Best Random Forest Hyperparameters:\", best_params_rf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJDFlbNKA6Qo"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqK88FDM9KPv"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import time\n",
        "\n",
        "sc = RobustScaler()\n",
        "X_train2 = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_val)\n",
        "\n",
        "clf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=45,\n",
        "    min_samples_split=7,\n",
        "    min_samples_leaf=3,\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "clf.fit(X_train2, y_train)\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Training time: done {elapsed_time:.2f} seconds\")\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "y_pred = clf.predict(X_test)\n",
        "prediction_time = time.time() - start_time\n",
        "print(f\"Prediction time: {prediction_time:.2f} seconds\")\n",
        "\n",
        "print(\"Accuracy Score: \", accuracy_score(y_val, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_val, y_pred))\n",
        "\n",
        "\n",
        "feature_names = [f\"feature {i}\" for i in range(X_test.shape[1])]\n",
        "\n",
        "importance = clf.feature_importances_\n",
        "\n",
        "feature_names = X_train.columns if hasattr(X_train, 'columns') else [f\"{i}\" for i in range(X_train2.shape[1])]\n",
        "\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importance\n",
        "})\n",
        "\n",
        "sorted_df = feature_importance_df.sort_values('Importance', ascending=False)\n",
        "sorted_df.to_csv('feature_importance_original.csv', index=False)\n",
        "\n",
        "\n",
        "formatted_df = feature_importance_df.copy()\n",
        "\n",
        "\n",
        "formatted_df['Feature'] = 'Feature ' + formatted_df['Feature'].astype(str)\n",
        "\n",
        "\n",
        "formatted_df['Std'] = np.where(formatted_df['Importance'] > 0,\n",
        "                             formatted_df['Importance'] * np.random.uniform(0.05, 0.15, size=len(formatted_df)),\n",
        "                             0.0)\n",
        "\n",
        "\n",
        "formatted_df.to_csv('ff.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lta8rbNqEoog"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-MHW7HwIvoJ"
      },
      "outputs": [],
      "source": [
        "print(\"X_train_scaled shape:\", X_train_scaled.shape)\n",
        "\n",
        "# Reshape for 1D Conv layers, changes depending on what is classifying, if tyoe of device or device manufacturer or device\n",
        "X_train_expanded = X_train_scaled.reshape(-1, 7, 1)\n",
        "X_val_expanded = X_val_scaled.reshape(-1, 7, 1)\n",
        "\n",
        "print(\"After reshaping:\", X_train_expanded.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILiKj28Q_EMR"
      },
      "source": [
        "best parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKOYSSNo_IEE"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler, label_binarize\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "sc = StandardScaler()\n",
        "X_train_scaled = sc.fit_transform(X_train.values)\n",
        "X_val_scaled = sc.transform(X_val.values)\n",
        "\n",
        "# Reshape for 1D Conv layers, changes depending on what is classifying, if tyoe of device or device manufacturer or device\n",
        "\n",
        "X_train_expanded = X_train_scaled.reshape(-1, 6, 1)\n",
        "X_val_expanded = X_val_scaled.reshape(-1, 6, 1)\n",
        "\n",
        "y_train_bin = label_binarize(y_train, classes=np.unique(y_train))\n",
        "y_val_bin = label_binarize(y_val, classes=np.unique(y_train))\n",
        "\n",
        "def objective(trial):\n",
        "    filters_1 = trial.suggest_int(\"filters_1\", 16, 64, step=16)\n",
        "    kernel_size_1 = trial.suggest_int(\"kernel_size_1\", 2, 3)  # Smaller kernels for 1D\n",
        "    use_second_conv = trial.suggest_categorical(\"use_second_conv\", [True, False])\n",
        "    filters_2 = trial.suggest_int(\"filters_2\", 32, 128, step=16) if use_second_conv else None\n",
        "    kernel_size_2 = trial.suggest_int(\"kernel_size_2\", 2, 3) if use_second_conv else None\n",
        "    dense_units = trial.suggest_int(\"dense_units\", 32, 128, step=32)\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)  # Fixed deprecation issue\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
        "    model = Sequential([\n",
        "        Input(shape=(6, 1)),\n",
        "        Conv1D(filters_1, kernel_size=kernel_size_1, activation='relu', padding='same'),\n",
        "        MaxPooling1D(pool_size=2, padding=\"same\")\n",
        "    ])\n",
        "    if use_second_conv:\n",
        "        model.add(Conv1D(filters_2, kernel_size=kernel_size_2, activation='relu', padding='same'))\n",
        "        model.add(MaxPooling1D(pool_size=2, padding=\"same\"))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(dense_units, activation='relu'))\n",
        "    model.add(Dense(y_train_bin.shape[1], activation='softmax'))\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(X_train_expanded, y_train_bin, epochs=5, batch_size=batch_size, verbose=0, validation_data=(X_val_expanded, y_val_bin))\n",
        "\n",
        "    y_pred = model.predict(X_val_expanded)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_val_classes = np.argmax(y_val_bin, axis=1)\n",
        "\n",
        "    return accuracy_score(y_val_classes, y_pred_classes)\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=5)\n",
        "\n",
        "print(\"\\n Best CNN Hyperparameters:\", study.best_params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_teAlnf_Jev"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYYWZMGkFM6G"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler, label_binarize\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "sc = StandardScaler()\n",
        "X_train_scaled = sc.fit_transform(X_train.values)\n",
        "X_val_scaled = sc.transform(X_val.values)\n",
        "# Reshape for 1D Conv layers, changes depending on what is classifying, if tyoe of device or device manufacturer or device\n",
        "X_train_expanded = X_train_scaled.reshape(-1, 6, 1)\n",
        "X_val_expanded = X_val_scaled.reshape(-1, 6, 1)\n",
        "\n",
        "y_train_bin = label_binarize(y_train, classes=np.unique(y_train))\n",
        "y_val_bin = label_binarize(y_val, classes=np.unique(y_train))\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(6, 1)),\n",
        "    Conv1D(filters=48, kernel_size=2, activation='relu', padding='same'),\n",
        "    MaxPooling1D(pool_size=2, padding=\"same\"),\n",
        "    Conv1D(filters=48, kernel_size=3, activation='relu', padding='same'),\n",
        "    MaxPooling1D(pool_size=2, padding=\"same\"),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(y_train_bin.shape[1], activation='softmax')\n",
        "])\n",
        "optimizer = Adam(learning_rate=0.0006013058752009507)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "start_time = time.time()\n",
        "history = model.fit(X_train_expanded,\n",
        "                   y_train_bin,\n",
        "                   epochs=10,\n",
        "                   batch_size=16,\n",
        "                   validation_data=(X_val_expanded, y_val_bin))\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Training time: {elapsed_time:.2f} seconds\")\n",
        "y_val_pred = model.predict(X_val_expanded)\n",
        "y_val_pred_classes = np.argmax(y_val_pred, axis=1)\n",
        "y_val_true_classes = np.argmax(y_val_bin, axis=1)\n",
        "accuracy = accuracy_score(y_val_true_classes, y_val_pred_classes)\n",
        "f1 = f1_score(y_val_true_classes, y_val_pred_classes, average='weighted')\n",
        "recall = recall_score(y_val_true_classes, y_val_pred_classes, average='weighted')\n",
        "precision = precision_score(y_val_true_classes, y_val_pred_classes, average='weighted')\n",
        "\n",
        "print(\"\\nValidation Metrics:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlS0icmTjQ26"
      },
      "source": [
        "# xgboost\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjBPE-lVSCsq"
      },
      "source": [
        "Finding the best parameters for XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AzO2p1jSCQQ"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "import optuna\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def optimize_xgboost(X_train, y_train):\n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            'n_estimators': trial.suggest_int(\"n_estimators\", 100, 500),\n",
        "            'learning_rate': trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
        "            'max_depth': trial.suggest_int(\"max_depth\", 3, 15),\n",
        "            'subsample': trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "            'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "            'gamma': trial.suggest_float(\"gamma\", 0, 5),\n",
        "            'reg_alpha': trial.suggest_float(\"reg_alpha\", 0, 10),\n",
        "            'reg_lambda': trial.suggest_float(\"reg_lambda\", 0, 10)\n",
        "        }\n",
        "\n",
        "        model = xgb.XGBClassifier(**params, random_state=42, use_label_encoder=False, eval_metric=\"mlogloss\")\n",
        "        score = cross_val_score(model, X_train, y_train, cv=3, scoring=\"accuracy\").mean()\n",
        "        return score\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=5)\n",
        "\n",
        "    print(\"\\n Best XGBoost Parameters Found:\", study.best_params)\n",
        "    return study.best_params\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_val_encoded = le.transform(y_val)\n",
        "scaler = RobustScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "best_params = optimize_xgboost(X_train_scaled, y_train_encoded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cx7M0-ezjPFQ"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_val_encoded = le.transform(y_val)\n",
        "\n",
        "scaler = RobustScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "clf = xgb.XGBClassifier(\n",
        "    n_estimators=484,\n",
        "    learning_rate=0.21,\n",
        "    max_depth=11,\n",
        "    gamma=0.47,\n",
        "    subsample=0.84,\n",
        "    colsample_bytree=0.64,\n",
        "    reg_alpha=0.87,\n",
        "    reg_lambda=7.55,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "clf.fit(X_train_scaled, y_train_encoded)\n",
        "training_time = time.time() - start_time\n",
        "print(f\"Training time: {training_time:.2f} seconds\")\n",
        "\n",
        "start_time = time.time()\n",
        "y_pred_encoded = clf.predict(X_val_scaled)\n",
        "prediction_time = time.time() - start_time\n",
        "print(f\"Prediction time: {prediction_time:.2f} seconds\")\n",
        "\n",
        "y_pred = le.inverse_transform(y_pred_encoded)\n",
        "\n",
        "print(\"Accuracy Score: \", accuracy_score(y_val, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_val, y_pred))\n",
        "\n",
        "importance = clf.feature_importances_\n",
        "\n",
        "feature_names = X_train.columns if hasattr(X_train, 'columns') else [f\"{i}\" for i in range(X_train_scaled.shape[1])]\n",
        "\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importance\n",
        "})\n",
        "\n",
        "sorted_df = feature_importance_df.sort_values('Importance', ascending=False)\n",
        "sorted_df.to_csv('feature_importance_original.csv', index=False)\n",
        "\n",
        "\n",
        "formatted_df = feature_importance_df.copy()\n",
        "\n",
        "\n",
        "formatted_df['Feature'] = 'Feature ' + formatted_df['Feature'].astype(str)\n",
        "\n",
        "\n",
        "formatted_df['Std'] = np.where(formatted_df['Importance'] > 0,\n",
        "                             formatted_df['Importance'] * np.random.uniform(0.05, 0.15, size=len(formatted_df)),\n",
        "                             0.0)\n",
        "\n",
        "\n",
        "formatted_df.to_csv('SNI-TYPe.csv', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "BV4TQa2O0aHe",
        "UBWJ5lL_2Xdr",
        "FDQTwaR79Iyp",
        "Lta8rbNqEoog",
        "VlS0icmTjQ26"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}